<!doctype html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, height=device-height, initial-scale=1.0" />
	<title>Alexander Popescu</title>
	<meta name="Why the state of consumer AI sucks" content="It does." />
	<style>
		body {
			margin: 5% auto;
			background: #f2f2f2;
			color: #444444;
			font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
			font-size: 16px;
			line-height: 1.8;
			text-shadow: 0 1px 0 #ffffff;
			max-width: 73%;
		}

		code {
			background: white;
		}

		a {
			border-bottom: 1px solid #444444;
			color: #444444;
			text-decoration: none;
		}


		a:hover {
			border-bottom: 0;
			background-color: #D9D9D9
		}

		img {
			width: 60%;
			height: auto;
			max-width: 400px;
			max-height: 400px;
			border: 2px solid #ddd;
			border-radius: 6px;
			padding: 8px;
			box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
		}
	</style>
</head>

<body>
	<header>
		<h1>Pope Askew</h1>
		<hr>

		<h3><a href="../index.html">Home</a></h3>
	</header>

	<h1>Consumer AI</h1>
	<p>Date</p>

	<p>
		Remember a few years ago when there was a large community online that would not stop talking about NFTs, Web3,
		and Cryptocurrency? Whatever happened to them? My theory is that once it became clear nobody was interested in a
		gambling-fueled internet, the members needed somewhere to pivot. Lucky for them, the AI hype train was right
		around the corner.
	</p>

	<h3>Overenthusiasm</h3>

	<p>
		Tech news is always going to have some crazy marketing strategies, but their newest buzzword has to be one of
		the funniest yet: AI. It has genuinely become a joke to me at this point, where I am pointing at my
		screen like Leonardo Dicaprio every time I see the mattress or ceiling fan company advertise their new product
		has
		"AI" in it.
	</p>
	<p>
		How did this sentiment develop? I, like most people, was blown away when ChatGPT launched in 2022. To roughly
		quote a comment I read somewhere online "It's like some researchers showed a CEO their new virtual dumbass
		thats constantly wrong, and the CEO gives them a great big thumbs up to shove it into every product."
	</p>
	<p>
		I don't think it used to be like this. Imagine a Tic-Tac-Toe AI script made 20 years ago. Chances are you are
		imagining a bot that never loses a match of Tic-Tac-Toe. This bot would likely have operated on a set of
		logical instructions made to guarentee that it never lost a game. This inability to lose no doubt bolstered its
		reputation, and I know this is a trivial example, but I think you could agree that before GPT's went mainstream
		the surrounding impression about AI agents was that they were very strong at performing their intended task.
	</p>
	<p>
		Now comes OpenAI. They are tackling a very difficult problem: Natural Language Processing (NLP). Language is
		hard, even with our billions of neurons it takes us nearly a year to utter our first words. Add to it some
		extreme nuance and the fact that language changes over time, and you can see how computer scientists were in bad
		company when coming up with
		a way for a computer to emulate this behavior. So instead, they decided to approximate it. It's very clever,
		really. Through fine tuning a series of neural networks over a long period of time, a program can create a model
		that predicts the next word in a sentence. Further testing showed that it was actually quite formidable at this
		task, and so the natural language processing problem was essentially solved.
	</p>

	<p>
		However, the same things that make this method so good at natural language processing, are the same that makes
		it fall short of other problems.
	</p>

	<h3>Inelegance in problem solving</h3>

	<p>
		When learning to program, most are told to avoid the so called "brute force" method. It is often inefficient,
		and sometimes impossible to conclude its operation, and is therefore deemed a bad practice.
	</p>
	<p>
		While not the same, I would categorize GPT's in a similar boat with brute forcing a problem. In same cases, it
		can be a good solution such as with NLP. However, at the end of the day, you are limited to whatever
		configuration of neuron weights you can achieve through pre-training your model. As far as I am aware, there is
		no way to perform open brain surgery on neural networks to make them perform in a desired way. They have
		essentially become black boxes similar to the brains inside our head, where we might be able to somewhat gauge
		what's going on, but nowhere near the extent to be able to influence it's behavior. </p>

	<p>
		The problems begin, in my opinion, when we try to brute force any problem we can't develop a solution for. For
		example, it seems that many use ChatGPT as a sort of arbiter of information. Whether that be directly through
		the OpenAI chat prompt or one of the many SaaS wrappers that exist (we will get to those later). ChatGPT, at its
		core, was only meant to process natural language. By the nature of it's statistical model, it has no conception
		of ideas, lies, or misinformation. It simply predicts the next word.
	</p>

	<p>
		This is what causes ChatGPT and similar
		chatbots to "lie" to you or otherwise spread misinformation. Those words had just happened to have the highest
		likelyhood of completing the sentence according to the pre trained model. Luckily, this particular complaint is
		not too bad. With some double checking, the feedback from chatgpt can be useful for generating some repetitive
		text file, or a giant lookup table of hard coded values for a programming project, etc. I think people will
		continue to liberally apply this technology to areas where much more elegant and therefore efficient algorithms
		can be utilised, but for now its not
		such a big deal in my opinion. Though I cannot say the same for...
	</p>

	<h3>Big AI wants your cat pictures</h3>

	<p>
		Data harvesting, you saw this one coming.
	</p>
</body>

</html>
